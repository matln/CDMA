"""
Copyright 2022 Jianchen Li
"""
import os
import sys
import time
import logging
import argparse
from rich.table import Table
from rich.console import Console

speakernet = os.getenv("speakernet")
sys.path.insert(0, os.path.dirname(speakernet))

from speakernet.backends.score_norm import normalize
from speakernet.utils.logging_utils import init_logger
from speakernet.utils.kaldi_common import StrToBoolAction
from speakernet.backends.cosine import compute_cosine_score
from speakernet.backends.get_eer_mindcf import compute_metric
from speakernet.backends.utils.average_cohort import average_spk_emb
from speakernet.backends.score_calibration import calibrate, evalute_calibration


logger = init_logger()

parser = argparse.ArgumentParser(
    description="""Score the specified trials and compute metrics per epochs.""",
    formatter_class=argparse.RawTextHelpFormatter,
    conflict_handler="resolve",
)
parser.add_argument(
    "--exp-dir", type=str, default="", help="Directory which is end with timestamp."
)

parser.add_argument(
    "--epochs",
    type=str,
    default="",
    help="epochs need to be scored. The default delimiter is ' '. ",
)

parser.add_argument("--position", type=str, default="near", help="near or far")

parser.add_argument(
    "--trials",
    type=str,
    default="",
    help="trials need to be scored. The default delimiter is ' '. ",
)

parser.add_argument("--evalset", type=str, default="voxceleb1_test", help="The evaluation set")

parser.add_argument(
    "--submean",
    type=str,
    action=StrToBoolAction,
    default=False,
    choices=["true", "false"],
    help="trials need to be scored. The default delimiter is ' '. ",
)

parser.add_argument(
    "--submean-set",
    type=str,
    default="voxceleb2_dev",
    help="The dataset used to compute the mean statistic",
)

parser.add_argument(
    "--score-method", type=str, default="cosine", choices=["cosine", "plda"], help="",
)

parser.add_argument(
    "--score-norm-method", type=str, default="", choices=["", "snorm", "asnorm"], help="",
)

parser.add_argument(
    "--top-n",
    type=int,
    default=100,
    help="Selecting only the top_n speakers with largest scores \n"
    "from the normalization cohort. Only for asnorm.",
)

parser.add_argument(
    "--cohort-set", type=str, default="voxceleb2_dev", help="The dataset used to normalize scores.",
)

parser.add_argument(
    "--average-cohort",
    type=str,
    action=StrToBoolAction,
    default=True,
    choices=["true", "false"],
    help="If True, average the embeddings from the same speaker to construct the imposter cohort.",
)

parser.add_argument(
    "--score-calibration",
    type=str,
    action=StrToBoolAction,
    default=False,
    choices=["true", "false"],
    help="",
)

parser.add_argument(
    "--cali-dev-set",
    type=str,
    default="",
    help="A dataset contains dev trials, which is used to train calibration model.",
)

parser.add_argument(
    "--cali-dev-trials",
    type=str,
    default="",
    help="The trials that used to train a calibration model.",
)

parser.add_argument(
    "--quality-measures", type=str, default="duration embedding imposter", help="",
)

parser.add_argument(
    "--force",
    type=str,
    action=StrToBoolAction,
    default=False,
    choices=["true", "false"],
    help="Whether to force the computation of the mean statistic \n" "and scoring of the trials",
)

parser.add_argument(
    "--ptarget", type=float, default=0.01, choices=[0.001, 0.01, 0.05], help="",
)

parser.add_argument(
    "--return-thresh",
    type=str,
    action=StrToBoolAction,
    default=False,
    choices=["true", "false"],
    help="Whether to return the threshold for computing eer and mindcf.",
)

parser.add_argument(
    "--compute-metric",
    type=str,
    action=StrToBoolAction,
    default=True,
    choices=["true", "false"],
    help="Whether to compute the eer and mindcf.",
)

args = parser.parse_args()

logger.info(f"Timestamp: {os.path.basename(args.exp_dir)}")

# Epochs that will be scored
epochs = args.epochs.split(" ") if args.epochs != "" else []
specified = True if args.epochs != "" else False

trials_files = args.trials.split(" ")

# Ensure command line param integrity.
if args.score_calibration:
    assert args.cali_dev_set != ""
    assert args.cali_dev_trials != ""
    assert args.quality_measures != ""
    quality_measures = args.quality_measures.split(" ")
    dev_quality_files = []
    eval_quality_files = []
    if "duration" in quality_measures:
        dev_quality_files = [f"data/{args.cali_dev_set}/quality_measures/{args.cali_dev_trials}_dur"]
else:
    quality_measures = []

# Results table. Generated by rich library
if not args.return_thresh:
    table = Table(
        title=f"Timestamp: {os.path.basename(args.exp_dir)}\n"
        f"evalset: {args.evalset}, submean: {args.submean}, "
        f"score norm: {True if args.score_norm_method != '' else False}\n"
        f"score calibration: {args.score_calibration}\n"
        f"EER \u2502 minDCF ({args.ptarget})"
    )
else:
    table = Table(
        title=f"Timestamp: {os.path.basename(args.exp_dir)}\n"
        f"evalset: {args.evalset}, submean: {args.submean}, "
        f"score norm: {True if args.score_norm_method != '' else False}\n"
        f"score calibration: {args.score_calibration}\n"
        f"EER (thresh) \u2502 minDCF (thresh) ({args.ptarget})"
    )
table.add_column("", justify="center", style="green", no_wrap=True)
styles = ["cyan", "magenta"]
for counter, trials_file in enumerate(trials_files):
    table.add_column(trials_file, justify="center", style=styles[counter % 2])
console = Console(record=True)

results_file_dir = f"{args.exp_dir}/results"
os.makedirs(results_file_dir, exist_ok=True)

results_file_suffix = ""
if args.submean:
    results_file_suffix += "_submean"
if args.score_norm_method == "snorm":
    results_file_suffix += "_snorm"
elif args.score_norm_method == "asnorm":
    results_file_suffix += f"_asnorm_top{args.top_n}"
if args.score_calibration:
    results_file_suffix += f"_qmf"
results_file = f"{results_file_dir}/{args.score_method}{results_file_suffix}.results"
fa = open(results_file, "a")

results = None


def get_extracted_epochs():
    extracted_epochs = []
    # emb_dir: {position}_epoch_{epoch}, e.g. near_epoch_30
    if os.path.exists(f"{args.exp_dir}/embeddings"):
        for emb_dir in os.listdir(f"{args.exp_dir}/embeddings"):
            if args.position == emb_dir.split("_")[0]:
                epoch = emb_dir.split("_")[2]
                evalset_vec_path = f"{args.exp_dir}/embeddings/{emb_dir}/{args.evalset}/xvector.scp"
                if os.path.exists(evalset_vec_path):
                    if args.submean:
                        submean_vec_path = (
                            f"{args.exp_dir}/embeddings/{emb_dir}/{args.submean_set}/xvector.scp"
                        )
                        if os.path.exists(submean_vec_path):
                            extracted_epochs.append(epoch)
                    else:
                        extracted_epochs.append(epoch)
    return extracted_epochs


def get_scored_epochs():
    scored_epochs = []
    # emb_dir: {position}_epoch_{epoch}, e.g. near_epoch_30
    if os.path.exists(f"{args.exp_dir}/embeddings"):
        for emb_dir in os.listdir(f"{args.exp_dir}/embeddings"):
            if args.position == emb_dir.split("_")[0]:
                epoch = emb_dir.split("_")[2]
                scored = True
                for trials_file in trials_files:
                    result_file = (
                        f"{args.exp_dir}/embeddings/{emb_dir}/{args.evalset}/scores/{args.score_method}_{trials_file}.result"
                        if not args.submean
                        else f"{args.exp_dir}/embeddings/{emb_dir}/{args.evalset}/scores/{args.score_method}_{trials_file}_submean.result"
                    )

                    if not os.path.exists(result_file):
                        scored = False
                if scored:
                    scored_epochs.append(epoch)
    return scored_epochs


def get_epochs():
    logger.info("Description: " + open(f"{args.exp_dir}/README", "r").read().strip())
    logger.info("Waiting for extracting embeddings...")
    while True:
        extracted_epochs = get_extracted_epochs()
        scored_epochs = get_scored_epochs()
        # Epochs that are not scored
        epochs = list(set(extracted_epochs) - set(scored_epochs))
        # Sort epoch
        epochs.sort(key=lambda x: float(x) if "." in x else float(x) + 1)

        if len(epochs) != 0:
            break
        else:
            time.sleep(10)
    return epochs


try:
    while True:
        # Find epochs that not be scored
        if len(epochs) == 0:
            epochs = get_epochs()
        logger.info(f"Score epochs: {' '.join(epochs)}")

        for epoch in epochs:
            # ------- Removing the embedding files if args.force is true. -------- #
            emb_dir = f"{args.exp_dir}/embeddings/{args.position}_epoch_{epoch}"
            mean_emb_path = f"{emb_dir}/{args.submean_set}/mean_emb.npy"
            if args.force and os.path.exists(mean_emb_path):
                os.remove(mean_emb_path)

            if args.average_cohort:
                cohort_embs_scp = f"{emb_dir}/{args.cohort_set}/spk_xvector.scp"
                if args.force and os.path.exists(cohort_embs_scp):
                    os.remove(cohort_embs_scp)
                    os.remove(cohort_embs_scp.replace(".scp", ".ark"))
            else:
                cohort_embs_scp = f"{emb_dir}/{args.cohort_set}/xvector.scp"

            # ----- ---- Scoring dev trials for evalset score calibration. ------- #
            if args.score_calibration:
                logger.info(
                    f"Scoring [bold red]{args.cali_dev_set}[/bold red] trials "
                    f"for epoch {epoch} [bold red]{args.evalset}[/bold red] "
                    f"calibration ...",
                    extra={"markup": True},
                )

                # Compute similarity
                cali_trials_path = f"data/{args.cali_dev_set}/trials/{args.cali_dev_trials}"
                if args.score_method == "plda":
                    raise NotImplementedError
                elif args.score_method == "cosine":
                    # Magnitude of embeddings
                    cali_dev_score, dev_emb_mgt = compute_cosine_score(
                        emb_dir,
                        args.cali_dev_set,
                        cali_trials_path,
                        args.submean,
                        args.submean_set,
                        args.force,
                        quality_measures
                    )
                    if "embedding" in quality_measures:
                        dev_quality_files.append(dev_emb_mgt)

                # Score nomalization
                if args.score_norm_method != "":
                    if args.average_cohort and not os.path.exists(cohort_embs_scp):
                        # Compute cohort speaker embeddings
                        logger.info(f"Computing cohort ({args.cohort_set}) speaker embeddings ...")
                        average_spk_emb(
                            f"data/{args.cohort_set}/spk2utt",
                            f"{emb_dir}/{args.cohort_set}/xvector.scp",
                            cohort_embs_scp,
                        )

                    # Normalize scores
                    logger.info("Normalizing scores ...")
                    cali_dev_score, dev_mean_imposter = normalize(
                        args.score_norm_method,
                        args.top_n,
                        cali_dev_score,
                        cohort_embs_scp,
                        f"{emb_dir}/{args.cali_dev_set}/xvector.scp",
                        mean_emb_path,
                        quality_measures,
                    )
                    if "imposter" in quality_measures:
                        dev_quality_files.append(dev_mean_imposter)

            # ------------------------ score eval trials ---------------------- #
            metrics = [epoch]
            for trials_file in trials_files:
                logger.info(
                    f"Scoring [bold red]{trials_file}[/bold red] for epoch {epoch} ...",
                    extra={"markup": True},
                )
                if "duration" in quality_measures:
                    eval_quality_files = [f"data/{args.evalset}/quality_measures/{trials_file}_dur"]

                # Compute similarity
                trials_path = f"data/{args.evalset}/trials/{trials_file}"
                if args.score_method == "plda":
                    raise NotImplementedError
                elif args.score_method == "cosine":
                    score_file, eval_emb_mgt = compute_cosine_score(
                        emb_dir,
                        args.evalset,
                        trials_path,
                        args.submean,
                        args.submean_set,
                        args.force,
                        quality_measures
                    )
                    if "embedding" in quality_measures:
                        eval_quality_files.append(eval_emb_mgt)

                # Score nomalization
                if args.score_norm_method != "":
                    if args.average_cohort and not os.path.exists(cohort_embs_scp):
                        # Compute cohort speaker embeddings
                        logger.info(f"Computing cohort ({args.cohort_set}) speaker embeddings ...")
                        average_spk_emb(
                            f"data/{args.cohort_set}/spk2utt",
                            f"{emb_dir}/{args.cohort_set}/xvector.scp",
                            cohort_embs_scp,
                        )

                    # Normalize scores
                    logger.info("Normalizing scores ...")
                    score_file, eval_mean_imposter = normalize(
                        args.score_norm_method,
                        args.top_n,
                        score_file,
                        cohort_embs_scp,
                        f"{emb_dir}/{args.evalset}/xvector.scp",
                        mean_emb_path,
                        quality_measures,
                    )
                    if "imposter" in quality_measures:
                        eval_quality_files.append(eval_mean_imposter)

                # Score calibration
                if args.score_calibration:
                    _mindcf, _actdcf, _cllr = evalute_calibration(
                        score_file, trials_path, args.ptarget
                    )
                    logger.info(f"Before calibration: minDCF {_mindcf:.4f}, "
                                f"actDCF {_actdcf:.4f}, cllr {_cllr:.4f}")

                    logger.info(f"Calibrating {trials_file} ...")

                    score_file = calibrate(
                        cali_trials_path,
                        [cali_dev_score],
                        [score_file],
                        dev_quality_files,
                        eval_quality_files,
                    )

                    _mindcf, _actdcf, _cllr = evalute_calibration(
                        score_file, trials_path, args.ptarget
                    )
                    logger.info(f"After calibration: minDCF {_mindcf:.4f}, "
                                f"actDCF {_actdcf:.4f}, cllr {_cllr:.4f}")

                # Compute EER and minDCF
                if args.compute_metric:
                    logger.info("Computing EER and minDCF ...")
                    metric = compute_metric(score_file, trials_path, args.ptarget, args.return_thresh)

                    metrics.append(metric)
            table.add_row(*metrics)

        console.print(table)
        results = console.export_text()

        if specified:
            fa.write(results + "\n")
            break
        else:
            epochs = []
except KeyboardInterrupt:
    if results is not None:
        fa.write(results + "\n")
